dir(cname) # Check that texts are in the directory
# Load the package for text mining and then load your texts into R
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Preprocessing text
docs <- tm_map(docs, removePunctuation) # Remove punctuation
docs <- tm_map(docs, removeNumbers) # Remove numbers
docs <- tm_map(docs, tolower) # Convert to lower case
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove stopwords
# Stemming - is this needed, necessary or advisable?
library(SnowballC)
docs <- tm_map(docs, stemDocument)
# Strip white space
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
# Staging the data
dtm <- DocumentTermMatrix(docs)
dtm
inspect(dtm[1:2, 1:20])
dim(dtm) # Shows the number of documents and terms
tdm <- TermDocumentMatrix(docs)   # Transpose of the original matrix
tdm
# Explore the data
# Organize terms by frequency
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
# Export matrix as csv
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="dtm_MP_P.csv")
# Remove sparse terms
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
inspect(dtms)
freq[head(ord)] # Least frequent
freq[tail(ord)] # Most frequent
head(table(freq), 20) # Check frequencies of frequencies low
tail(table(freq), 20) # high
freq <- colSums(as.matrix(dtms))   # Frequency of sparse terms
freq
findFreqTerms(dtm, lowfreq=50) # Identifying terms which appear frequently
wf <- data.frame(word=names(freq), freq=freq)   # Alternative form to line above
head(wf)
# Plot words which appear at least 500 times - can reduce but lose clarity in plot
library(ggplot2)
p <- ggplot(subset(wf, freq>500), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
# Relationships between terms
# Term Correlations
findAssocs(dtm, c("independent" , "miss"), corlimit=0.98)
# specifying a correlation limit of 0.98   - Not sure how useful this is
# Word Cloud
library(wordcloud)
set.seed(142)
wordcloud(names(freq), freq, min.freq=50)
set.seed(142)
wordcloud(names(freq), freq, min.freq=100)
# as above with colour
set.seed(142)
wordcloud(names(freq), freq, min.freq=100, scale=c(5, .1),
colors=brewer.pal(6, "Dark2"))   # 25 is too small!!!
x <- state.x77[1:5,]
x
sort(x$Population)
sort(x[,1])
sort(x)
library(lattice)
barplot(state.x77[1:5, 1])
barplot(state.x77[1:10, 1])
main="Population by state")
barplot(state.x77[1:10, 1], ylab="Population", xlab="State", main="Population by state")
barplot(state.x77[1:10, 1], ylab="Population", xlab="State",
main="Population by state", col=1:10)
plot(mtcars$disp, mtcars$mpg)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG compared with engine displacement")
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1)
head(USArrests)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1, col = 1:5)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1, col = 1:5, pch=20)
hist(USArrests)
hist(USArrests$Assault)
head(state.x77)
hist(state.x77[1:10,3])
hist(state.x77[,3])
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length~Species)
boxplot(iris$Sepal.Length ~ iris$Species)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=2)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=2)
plot(iris[,-5], pch=20, col=iris[,5], cex=2)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=1)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=6)
head(quakes)
xyplot(lat~long, data=quakes, pch=".")
xyplot(lat~long, data=quakes, pch=".", main="Earthquakes in the Pacific Ocean (since 1964)")
xyplot(lat~long, data=quakes, pch="*", main="Earthquakes in the Pacific Ocean (since 1964)")
head(barley)
barchart(yield ~ variety | site, data = barley,
groups = year, layout = c(1,6),
ylab = "Barley Yield (bushels/acre)",
scales = list(x = list(abbreviate = TRUE,
minlength = 5)))
scales = list(x = list(abbreviate = F,
barchart(yield ~ variety | site, data = barley,
groups = year, layout = c(1,6),
ylab = "Barley Yield (bushels/acre)",
scales = list(x = list(abbreviate = F,
minlength = 5)))
dotplot(variety ~ yield | year * site, data=barley)
dotplot(variety ~ yield | site, data = barley, groups = year,
key = simpleKey(levels(barley$year), space = "right"),
xlab = "Barley Yield (bushels/acre) ",
aspect=0.5, layout = c(1,6), ylab=NULL)
head(singer)
hist(singer$height)
hist(singer$height ~ singer$voice.part)
histogram( ~ height | voice.part, data = singer, nint = 17,
endpoints = c(59.5, 76.5), layout = c(2,4), aspect = 1,
xlab = "Height (inches)")
histogram( ~ height | voice.part, data = singer,
endpoints = c(59.5, 76.5), layout = c(2,4), aspect = 1,
xlab = "Height (inches)")
stripplot(voice.part ~ jitter(height), data = singer, aspect = 1,
jitter = TRUE, xlab = "Height (inches)")
stripplot(voice.part ~ jitter(height), data = singer, aspect = 1,
jitter = TRUE, xlab = "Height (inches)")
head(chickwts)
hist(chickwts$weight)
hist(chickwts$weight, col="red", xlab="Weight", main = "Histogram of Chick Weights")
data()
xyplot(Sepal.Length + Sepal.Width ~ Petal.Length + Petal.Width | Species,
data = iris, scales = "free", layout = c(2, 2),
auto.key = list(x = .6, y = .7, corner = c(0, 0)))
library(lattice)
xyplot(Sepal.Length + Sepal.Width ~ Petal.Length + Petal.Width | Species,
data = iris, scales = "free", layout = c(2, 2),
auto.key = list(x = .6, y = .7, corner = c(0, 0)))
wireframe(volcano, shade = TRUE,
aspect = c(61/87, 0.4),
light.source = c(10,0,10))
head(volcano)
x <- c(66.5,22,58.5,70.5,67.5,51,68.5,60,63.5,87,62,58.5,61.5,68.5,63.5,56.5,55,51,54.5)
x <- c(66.5,22,58.5,70.5,67.5,51,68.5,60,63.5,87,62,58.5,61.5,68.5,63.5,56.5,55,51,54.5)
sort(x)
y <-sort(x)
plot(y)
hist(y)
hist(y[-1])
hist(y[-1], breaks=10)
hist(y[-1], breaks=20)
mtcars[which(mtcars$disp == 472), ]
library(tm) # Load package
setwd("~/PhD_Main/Pilot")
setwd("~/PhD_Main/Pilot/PhD_Pilot/Data/Plain_Texts/Austen")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
docs <-tm_map(docs,content_transformer(tolower))
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Good practice to check every now and then
writeLines(as.character(docs[[3]]))
#Stem document
docs <- tm_map(docs,stemDocument)
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
install.packages("topicmodels")
library(topicmodels)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 20
lda_out <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed,
best=best, burnin = burnin, iter = iter, thin=thin))
lda_out_topics <- as.matrix(topics(lda_out))
write.csv(lda_out_topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
lda_out_terms <- as.matrix(terms(lda_out,6))
write.csv(lda_out_terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))
topic_probabilities <- as.data.frame(lda_out@gamma)
write.csv(topic_probabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))
topic_1_to_topic_2 <- lapply(1:nrow(dtm),function(x)
sort(topic_probabilities[x,])[k]/sort(topic_probabilities[x,])[k-1])
topic_2_to_topic_3 <- lapply(1:nrow(dtm),function(x)
sort(topic_probabilities[x,])[k-1]/sort(topic_probabilities[x,])[k-2])
write.csv(topic_1_to_topic_2,file=paste("LDAGibbs",k,"Topic1ToTopic2.csv"))
write.csv(topic_2_to_topic_3,file=paste("LDAGibbs",k,"Topic2ToTopic3.csv"))
lda_out_topics
lda_out_terms
head(lda_out)
head(freq)
head(dtm)
devtools::install_github("nikita-moor/ldatuning")
library("ldatuning")
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 20, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 2L,
verbose = TRUE
)
FindTopicsNumber_plot(result)
setwd("~/PhD_Main/Pilot")
input_dir <- "PhD_Pilot/Data/Plain_Texts/Austen"
text_files <- dir(path = input_dir, pattern = "*.txt")
library(koRpus)
install.packages(koRpus)
install.packages("koRpus")
library(koRpus)
i <- 1
tagged_text <-treetag(file.path(input_dir, text_files[i]), treetagger = "manual",
lang = "en", TT.options = list(path = "TreeTagger",
preset = "en"))
head(taggedText(text_tagged))
head(taggedText(tagged_text))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
head(nouns)
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
head(collapse_nouns)
output_dir <- "PhD_Pilot/Data/Plain_Texts/Austen/Collapsed"
write(collapse_nouns, file.path(output_dir), file = text_files[i])
write(collapse_nouns,file = "PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/SS.txt")
noun_files <- list()
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
}
input_dir <- "PhD_Pilot/Data/Plain_Texts/Austen"
text_files <- dir(path = input_dir, pattern = "*.txt")
noun_files <- list()
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
write(noun_files[i], file=paste0("PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/",
names(noun_files)[i], ".txt"))
}
input_dir <- "PhD_Pilot/Data/Plain_Texts/Austen"
text_files <- dir(path = input_dir, pattern = "*.txt")
noun_files <- list()
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
}
for(i in 1:length(noun_files)) {
write(noun_files[i], file=paste0("PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/",
names(noun_files)[i], ".txt"))
}
for(i in 1:length(noun_files)) {
write.csv(noun_files[i], file=paste0("PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/",
names(noun_files)[i], ".txt"))}
input_dir <- "PhD_Pilot/Data/Plain_Texts/Austen"
output_dir <- "PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/"
text_files <- dir(path = input_dir, pattern = "*.txt")
noun_files <- list()
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
}
library(koRpus)
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
}
for(i in 1:length(noun_files)) {
write.csv(noun_files[i], file=paste0(output_dir,
names(noun_files)[i], ".txt"))
}
extract_nouns <- function(input_dir, output_dir){
text_files <- dir(path = input_dir, pattern = "*.txt")
noun_files <- list()
for(i in 1:length(text_files)){
tagged_text <-treetag(file.path(input_dir, text_files[i]),
treetagger = "manual",lang = "en",
TT.options = list(path = "TreeTagger",preset = "en"))
tagged_doc <- tagged_text@TT.res[, c(1, 2, 6)]
single_nouns <- subset(tagged_doc, tag == "NN")
plural_nouns <- subset(tagged_doc, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
text_nouns <- nouns
collapse_nouns <- paste(text_nouns, collapse = " ")
noun_files[[text_files[i]]] <- collapse_nouns
}
for(i in 1:length(noun_files)) {
write.csv(noun_files[i], file=paste0(output_dir,
names(noun_files)[i], ".txt"))
}
return("Files Saved")
}
library(koRpus)
input_dir <- "PhD_Pilot/Data/Plain_Texts/Austen"
output_dir <- "PhD_Pilot/Data/Plain_Texts/Austen/Collapsed/"
extract_nouns(input_dir, output_dir)
library(tm) # Load package
setwd("~/PhD_Main/Pilot/PhD_Pilot/Data/Plain_Texts/Austen/Collapsed")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
writeLines(as.character(docs[[3]]))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[3]]))
docs <- tm_map(docs,stemDocument)
dtm <- DocumentTermMatrix(docs)
#Good practice to check every now and then
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
write.csv(freq[ord],"ja_collapsed_word_freq.csv")
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 8
Run LDA using Gibbs sampling
lda_out <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed,
best=best, burnin = burnin, iter = iter, thin=thin))
# This takes lots of time!
library(topicmodels)
Run LDA using Gibbs sampling
lda_out <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed,
best=best, burnin = burnin, iter = iter, thin=thin))
# This takes lots of time!
lda_out_topics <- as.matrix(topics(lda_out))
write.csv(lda_out_topics,file=paste("LDAGibbs",k,"DocsToTopics_nouns.csv"))
#top 6 terms in each topic
lda_out_terms <- as.matrix(terms(lda_out,6))
write.csv(lda_out_terms,file=paste("LDAGibbs",k,"TopicsToTerms_nouns.csv"))
#probabilities associated with each topic assignment
topic_probabilities <- as.data.frame(lda_out@gamma)
write.csv(topic_probabilities,file=paste("LDAGibbs",k,"TopicProbabilities_nouns.csv"))
#Find relative importance of top 2 topics
topic_1_to_topic_2 <- lapply(1:nrow(dtm),function(x)
sort(topic_probabilities[x,])[k]/sort(topic_probabilities[x,])[k-1])
#Find relative importance of second and third most important topics
topic_2_to_topic_3 <- lapply(1:nrow(dtm),function(x)
sort(topic_probabilities[x,])[k-1]/sort(topic_probabilities[x,])[k-2])
#write to file
write.csv(topic_1_to_topic_2,file=paste("LDAGibbs",k,"Topic1ToTopic2_nouns.csv"))
write.csv(topic_2_to_topic_3,file=paste("LDAGibbs",k,"Topic2ToTopic3_nouns.csv"))
lda_out_terms
lda_out_topics
topic_probabilities
library(tm) # Load package
setwd("~/PhD_Main/Pilot/PhD_Pilot/Data/Plain_Texts/Austen")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
writeLines(as.character(docs[[2]]))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[2]]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
writeLines(as.character(docs[[2]]))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[2]]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[2]]))
docs <- tm_map(docs,stemDocument)
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
write.csv(freq[ord],"ja_word_freq.csv")
library(topicmodels)
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
inspect(dtms)
freq2 <- colSums(as.matrix(dtms))
freq2
findFreqTerms(dtm, lowfreq=50)
findFreqTerms(dtm, lowfreq=100)
library(ggplot2)
findFreqTerms(dtm, lowfreq=200)
findFreqTerms(dtm, lowfreq=250)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
wf <- data.frame(word=names(freq2), freq=freq)
wf <- data.frame(word=names(freq2), freq=freq2)
head(wf)
p <- ggplot(subset(wf, freq>250), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
library(ggplot2)
p <- ggplot(subset(wf, freq>500), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
set.seed(142)
wordcloud(names(freq), freq, min.freq=500)
set.seed(142)
wordcloud(names(freq), freq, min.freq=500, scale=c(5, .1),
colors=brewer.pal(6, "Dark2"))
dtmss <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space, maximum.
inspect(dtmss)
library(cluster)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward")
fit
plot(fit, hang=-1)
findAssocs(dtm, "marriage", 0.2)
findAssocs(dtm, "independ", 0.2)
findAssocs(dtm, "marri", 0.2)
install.packages("timevis")
library(timevis)
timevis()
