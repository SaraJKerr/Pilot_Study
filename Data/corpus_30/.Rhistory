plural_nouns <- subset(tagged_doc6, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[6]))
single_nouns <- subset(tagged_doc7, tag == "NN")
plural_nouns <- subset(tagged_doc7, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[7]))
single_nouns <- subset(tagged_doc8, tag == "NN")
plural_nouns <- subset(tagged_doc8, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[8]))
single_nouns <- subset(tagged_doc9, tag == "NN")
plural_nouns <- subset(tagged_doc9, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[9]))
single_nouns <- subset(tagged_doc10, tag == "NN")
plural_nouns <- subset(tagged_doc10, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[10]))
single_nouns <- subset(tagged_doc11, tag == "NN")
plural_nouns <- subset(tagged_doc11, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[11]))
single_nouns <- subset(tagged_doc12, tag == "NN")
plural_nouns <- subset(tagged_doc12, tag == "NNS")
nouns <- rbind(single_nouns, plural_nouns)
nouns <- nouns$token
# Save noun file as plain text
write(nouns, file = paste0("Data/Tagged/", filename[12]))
proper_single_nouns <- subset(tagged_doc1, tag == "NP")
proper_plural_nouns <- subset(tagged_doc1, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[1]))
proper_single_nouns <- subset(tagged_doc2, tag == "NP")
proper_plural_nouns <- subset(tagged_doc2, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[2]))
proper_single_nouns <- subset(tagged_doc3, tag == "NP")
proper_plural_nouns <- subset(tagged_doc3, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[3]))
proper_single_nouns <- subset(tagged_doc4, tag == "NP")
proper_plural_nouns <- subset(tagged_doc4, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[4]))
proper_single_nouns <- subset(tagged_doc5, tag == "NP")
proper_plural_nouns <- subset(tagged_doc5, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[5]))
proper_single_nouns <- subset(tagged_doc6, tag == "NP")
proper_plural_nouns <- subset(tagged_doc6, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[6]))
proper_single_nouns <- subset(tagged_doc7, tag == "NP")
proper_plural_nouns <- subset(tagged_doc7, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[7]))
proper_single_nouns <- subset(tagged_doc8, tag == "NP")
proper_plural_nouns <- subset(tagged_doc8, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[8]))
proper_single_nouns <- subset(tagged_doc9, tag == "NP")
proper_plural_nouns <- subset(tagged_doc9, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[9]))
proper_single_nouns <- subset(tagged_doc10, tag == "NP")
proper_plural_nouns <- subset(tagged_doc10, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[10]))
proper_single_nouns <- subset(tagged_doc11, tag == "NP")
proper_plural_nouns <- subset(tagged_doc11, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[11]))
proper_single_nouns <- subset(tagged_doc12, tag == "NP")
proper_plural_nouns <- subset(tagged_doc12, tag == "NPS")
proper_nouns <- rbind(proper_single_nouns, proper_plural_nouns)
proper_nouns <- proper_nouns$token
# Save proper noun file as plain text
write(proper_nouns, file = paste0("Data/Proper_Nouns/", filename[12]))
library(tm)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Tagged")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[1]]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
freq[ord]
write.csv(freq[ord], "Results/Word_freq_tag.csv")
setwd("~/PhD_Main/Pilot/Pilot_Study")
write.csv(freq[ord], "Results/Word_freq_tag.csv")
library(tm)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Tagged")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[1]]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
setwd("~/PhD_Main/Pilot/Pilot_Study")
write.csv(freq[ord], "Results/Word_freq_tag_st.csv")
library(ldatuning)
library(topicmodels)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 42),
mc.cores = 2L, # Check this with About this Mac
verbose = TRUE
)
FindTopicsNumber_plot(result)
View(result)
burnin <- 500
iter <- 2000
thin <- 500
seed <-list(123, 420, 224, 10098, 49)
nstart <- 5
best <- TRUE
k <- 11 # number suggested from ldatuning
ldaOut <-LDA(dtm, k, method = "Gibbs", control = list(nstart = nstart,
seed = seed, best = best, burnin = burnin, iter = iter,
thin = thin))
ldaOut_topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut_topics, file = paste0("Data/LDAGibbs",k,"_Tagged_DocsToTopics.csv"))
ldaOut_terms <- as.matrix(terms(ldaOut,6))
write.csv(ldaOut_terms, file = paste0("LDAGibbs",k,"_Tagged_TopicsToTerms.csv"))
View(ldaOut_terms)
View(ldaOut_topics)
my_stopwords <- c("a", "able", "about", "across","after", "all", "almost", "also",
"am", "among", "an", "and", "any", "are", "as", "at", "be",
"because", "been", "but", "by", "can", "cannot", "could", "dear",
"did", "do", "does", "either", "else", "ever", "every", "for",
"from", "get", "got","had","has", "have", "he", "her", "hers",
"him", "his", "how", "however", "i", "if", "in", "into", "is",
"it", "its", "just", "least", "let", "like", "likely", "may",
"me", "might", "most", "must", "my", "neither", "no", "nor","not",
"of", "off", "often", "on", "only", "or", "other", "our","own",
"rather", "said", "say","says","she", "should", "since", "so",
"some", "than", "that", "the", "their", "them", "then", "there",
"these", "they", "this", "tis", "to", "too", "twas", "us", "wants",
"was", "we", "were", "what", "when", "where", "which", "while",
"who", "whom", "why", "will", "with", "would", "yet", "you",
"your")
ldaOut20 <-as.matrix(terms(ldaOut, 6))
View(ldaOut_topics)
View(ldaOut20)
ldaOut20 <-as.matrix(terms(ldaOut, 20))
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
install.packages("LDAvis")
install.packages("LDAvis")
ldaOut %>%
topicmodels2LDAvis() %>%
LDAvis::serVis()
library(magrittr)
ldaOut %>%
topicmodels2LDAvis() %>%
LDAvis::serVis()
setwd("~/PhD_Main/Pilot/Pilot_Study")
ldaOut %>%
topicmodels2LDAvis() %>%
LDAvis::serVis(out.dir = "Plots/LDAvis_Tagged")
View(ldaOut_terms)
library(tm)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Texts")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
head(docs)
head(docs[1])
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[1]]))
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[1]]))
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
library(topicmodels)
burnin <- 500
iter <- 2000
thin <- 500
seed <-list(123, 420, 224, 10098, 49)
nstart <- 5
best <- TRUE
# Number of topics
k <- 11
ldaOut <-LDA(dtm, k, method = "Gibbs", control = list(nstart = nstart,
seed = seed, best = best, burnin = burnin, iter = iter,
thin = thin))
ldaOut_topics <- as.matrix(topics(ldaOut))
ldaOut_terms <- as.matrix(terms(ldaOut,6))
View(ldaOut_terms)
View(ldaOut_topics)
setwd("~/PhD_Main/Pilot/Pilot_Study")
write.csv(ldaOut_topics, file = paste0("Data/LDAGibbs",k,"_DocsToTopics.csv"))
write.csv(ldaOut_terms, file = paste0("LDAGibbs",k,"_TopicsToTerms.csv"))
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
ldaOut %>%
topicmodels2LDAvis() %>%
LDAvis::serVis(out.dir = "Plots/LDAvis1")
View(ldaOut_terms)
View(ldaOut_topics)
topicProbabilities <- as.data.frame(ldaOut@gamma)
ldaOut_terms <- as.matrix(terms(ldaOut,6))
write.csv(ldaOut_terms, file = paste0("Data/LDAGibbs",k,"_TopicsToTerms.csv"))
write.csv(topicProbabilities,file=paste("Data/LDAGibbs",k,"TopicProbabilities.csv"))
text_1 <- readLines(filenames[1:6])
text_1 <- lapply(filenames[1:6],readLines)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Texts")
filenames <- list.files(getwd(),pattern="*.txt")
text_1 <- lapply(filenames[1:6],readLines)
text_2 <- lapply(filenames[7:12],readLines)
require(tm)
text_1c <- Corpus(VectorSource(text_1))
text2_c <- Corpus(VectorSource(text_2))
text_1c <- tm_map(text_1c, stripWhitespace)
text1c <- tm_map(text_1c, content_transformer(tolower))
text_1c <- tm_map(text_1c, removePunctuation)
text_1c <- tm_map(text_1c, removeNumbers)
text2_c <- tm_map(text2_c, stripWhitespace)
text2_c <- tm_map(text2_c, content_transformer(tolower))
text2_c <- tm_map(text2_c, removePunctuation)
text2_c <- tm_map(text2_c, removeNumbers)
dtm_1 <- DocumentTermMatrix(text_1c)
dtm_2 <- DocumentTermMatrix(text2_c)
df_1 <- data.frame(as.matrix(dtm_1))
df_2 <- data.frame(as.matrix(dtm_2))
author_1 <- as.matrix(sort(sapply(df_1, "sum"), decreasing = T)
[1:length(df_1)], colnames = count)
author_2 <- as.matrix(sort(sapply(df_2, "sum"), decreasing = T)
[1:length(df_2)], colnames = count)
author_1 <- author_1[complete.cases(author_1),]
author_2 <- author_2[complete.cases(author_2),]
words_author_1 <- data.frame(author_1)
words_author_2 <- data.frame(author_2)
words_compare <- merge(words_author_1, words_author_2, by="row.names", all = T)
words_compare[is.na(words_compare)] <- 0
words_compare$prop <- words_compare$author_1/sum(words_compare$author_1)
words_compare$prop2 <- words_compare$author_2/sum(words_compare$author_2)
a <- words_compare$prop
b <- words_compare$prop2
c <- words_compare$author_1
d <- words_compare$author_2
e <- sum(c)
f <- sum(d)
words_compare$z <- (a - b) / ((sqrt(((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d)) * (1 - ((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d))))) * (sqrt((sum(c) + sum(d)) / (sum(c) *
sum(d)))))
words_compare <- words_compare[order(abs(words_compare$z), decreasing = T),]
words_compare$dif1 <- -100 * (1 - words_compare$prop/words_compare$prop2)
words_compare$dif2 <- 100 * (1 - words_compare$prop2/words_compare$prop)
words_compare$dif <- 0
words_compare$dif[words_compare$dif1 < 0] <- words_compare$dif1[words_compare$dif1 < 0]
words_compare$dif[words_compare$dif2 > 0] <- words_compare$dif2[words_compare$dif2 > 0]
words_compare$dif1 <- NULL
words_compare$dif2 <- NULL
require(ggplot2)
words_compare$z2 <- 0 # insignificant terms
words_compare$z2[abs(words_compare$z) >= 1.96] <- 1 # significant at 95% confidence
words_compare$z2[abs(words_compare$z) >= 2.58] <- 2 # significant at 99% confidence
words_compare$z2[abs(words_compare$z) >= 3.08] <- 3 # significant at 99.79%
words_compare <- words_compare[words_compare$dif >-99 & words_compare$dif <99,]
words_compare <- words_compare[order(abs(words_compare$prop2+words_compare$prop),
decreasing = T),]
ggplot(words_compare, aes(z, dif, colour=z2, label=Row.names)) + geom_point()+
scale_colour_gradientn(name="Z Score", labels=c("Not significant", "1.96", "2.58", "3.08"),colours=rainbow(4))+
geom_text(aes(label=ifelse(words_compare$z == max(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
geom_text(aes(label=ifelse(words_compare$z == min(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
ylab("Difference")+
xlab("Z Score")+
#theme(panel.background = element_rect(colour =  = "pink"))+
theme(panel.background = element_rect(fill = "black"))+
ggtitle("Relationship between z score and percentage difference \n in Austen and Edgeworth")
ggplot(head(words_compare, 100), aes(dif, log(abs(author_1 + author_2)),size = (author_1 + author_2),label=Row.names, colour = z2, position_jitter(w=0.002, h=0.002)))+
scale_colour_gradientn(colours=rainbow(4))+
geom_text(fontface = 2, alpha = .8) +
scale_size(range = c(2, 10)) +
ylab("log number of mentions") +
xlab("Percentage difference between samples \n <-------More in Edgeworth --------------|--------More in Austen----------->") +
geom_vline(xintercept=0, colour  = "red", linetype=2)+
theme_bw() + theme(legend.position = "none") +
ggtitle("Differences in Terms Used by \n Austen and Edgeworth")
text_1c <- tm_map(text_1c, removeWords, stopwords("english")) # Remove 1st # to activate
text_1c <- tm_map(text_1c, stemDocument) # Remove 1st # to activate
text2_c <- tm_map(text2_c, removeWords, stopwords("english")) # Remove 1st # to activate
text2_c <- tm_map(text2_c, stemDocument) # Remove 1st # to activate
dtm_1 <- DocumentTermMatrix(text_1c)
dtm_2 <- DocumentTermMatrix(text2_c)
df_1 <- data.frame(as.matrix(dtm_1))
df_2 <- data.frame(as.matrix(dtm_2))
author_1 <- as.matrix(sort(sapply(df_1, "sum"), decreasing = T)
[1:length(df_1)], colnames = count)
author_2 <- as.matrix(sort(sapply(df_2, "sum"), decreasing = T)
[1:length(df_2)], colnames = count)
author_1 <- author_1[complete.cases(author_1),]
author_2 <- author_2[complete.cases(author_2),]
# Creates data frame
words_author_1 <- data.frame(author_1)
words_author_2 <- data.frame(author_2)
# Merge the two tables by row names
words_compare <- merge(words_author_1, words_author_2, by="row.names", all = T)
# Replace NA with 0
words_compare[is.na(words_compare)] <- 0
words_compare$prop <- words_compare$author_1/sum(words_compare$author_1)
words_compare$prop2 <- words_compare$author_2/sum(words_compare$author_2)
a <- words_compare$prop
b <- words_compare$prop2
c <- words_compare$author_1
d <- words_compare$author_2
e <- sum(c)
f <- sum(d)
words_compare$z <- (a - b) / ((sqrt(((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d)) * (1 - ((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d))))) * (sqrt((sum(c) + sum(d)) / (sum(c) *
sum(d)))))
words_compare <- words_compare[order(abs(words_compare$z), decreasing = T),]
words_compare$dif1 <- -100 * (1 - words_compare$prop/words_compare$prop2)
# Calculate percentage increase:
words_compare$dif2 <- 100 * (1 - words_compare$prop2/words_compare$prop)
words_compare$dif <- 0
words_compare$dif[words_compare$dif1 < 0] <- words_compare$dif1[words_compare$dif1 < 0]
words_compare$dif[words_compare$dif2 > 0] <- words_compare$dif2[words_compare$dif2 > 0]
words_compare$dif1 <- NULL
words_compare$dif2 <- NULL
require(ggplot2)
words_compare$z2 <- 0 # insignificant terms
words_compare$z2[abs(words_compare$z) >= 1.96] <- 1 # significant at 95% confidence
words_compare$z2[abs(words_compare$z) >= 2.58] <- 2 # significant at 99% confidence
words_compare$z2[abs(words_compare$z) >= 3.08] <- 3 # significant at 99.79%
words_compare <- words_compare[words_compare$dif >-99 & words_compare$dif <99,]
words_compare <- words_compare[order(abs(words_compare$prop2+words_compare$prop),
decreasing = T),]
ggplot(words_compare, aes(z, dif, colour=z2, label=Row.names)) + geom_point()+
scale_colour_gradientn(name="Z Score", labels=c("Not significant", "1.96", "2.58", "3.08"),colours=rainbow(4))+
geom_text(aes(label=ifelse(words_compare$z == max(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
geom_text(aes(label=ifelse(words_compare$z == min(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
ylab("Difference")+
xlab("Z Score")+
#theme(panel.background = element_rect(colour =  = "pink"))+
theme(panel.background = element_rect(fill = "black"))+
ggtitle("Relationship between z score and percentage difference \n in Austen and Edgeworth")
ggplot(head(words_compare, 100), aes(dif, log(abs(author_1 + author_2)),size = (author_1 + author_2),label=Row.names, colour = z2, position_jitter(w=0.002, h=0.002)))+
scale_colour_gradientn(colours=rainbow(4))+
geom_text(fontface = 2, alpha = .8) +
scale_size(range = c(2, 10)) +
ylab("log number of mentions") +
xlab("Percentage difference between samples \n <-------More in Edgeworth --------------|--------More in Austen----------->") +
geom_vline(xintercept=0, colour  = "red", linetype=2)+
theme_bw() + theme(legend.position = "none") +
ggtitle("Differences in Terms Used by \n Austen and Edgeworth")
text_1 <- lapply(filenames[1:6],readLines)
text_2 <- lapply(filenames[7:12],readLines)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Texts")
filenames <- list.files(getwd(),pattern="*.txt")
text_1 <- lapply(filenames[1:6],readLines)
text_2 <- lapply(filenames[7:12],readLines)
require(tm)
text_1c <- Corpus(VectorSource(text_1))
text2_c <- Corpus(VectorSource(text_2))
text_1c <- tm_map(text_1c, stripWhitespace)
text1c <- tm_map(text_1c, content_transformer(tolower))
text_1c <- tm_map(text_1c, removePunctuation)
text_1c <- tm_map(text_1c, removeNumbers)
text2_c <- tm_map(text2_c, stripWhitespace)
text2_c <- tm_map(text2_c, content_transformer(tolower))
text2_c <- tm_map(text2_c, removePunctuation)
text2_c <- tm_map(text2_c, removeNumbers)
dtm_1 <- DocumentTermMatrix(text_1c)
dtm_2 <- DocumentTermMatrix(text2_c)
df_1 <- data.frame(as.matrix(dtm_1))
df_2 <- data.frame(as.matrix(dtm_2))
author_1 <- as.matrix(sort(sapply(df_1, "sum"), decreasing = T)
[1:length(df_1)], colnames = count)
author_2 <- as.matrix(sort(sapply(df_2, "sum"), decreasing = T)
[1:length(df_2)], colnames = count)
author_1 <- author_1[complete.cases(author_1),]
author_2 <- author_2[complete.cases(author_2),]
author_1 <- author_1[complete.cases(author_1),]
author_2 <- author_2[complete.cases(author_2),]
words_author_1 <- data.frame(author_1)
words_author_2 <- data.frame(author_2)
words_compare <- merge(words_author_1, words_author_2, by="row.names", all = T)
words_compare[is.na(words_compare)] <- 0
words_compare$prop <- words_compare$author_1/sum(words_compare$author_1)
words_compare$prop2 <- words_compare$author_2/sum(words_compare$author_2)
a <- words_compare$prop
b <- words_compare$prop2
c <- words_compare$author_1
d <- words_compare$author_2
e <- sum(c)
f <- sum(d)
words_compare$z <- (a - b) / ((sqrt(((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d)) * (1 - ((sum(c) * a) + (sum(d) * b)) / (sum(c) +
sum(d))))) * (sqrt((sum(c) + sum(d)) / (sum(c) *
sum(d)))))
words_compare <- words_compare[order(abs(words_compare$z), decreasing = T),]
words_compare$dif1 <- -100 * (1 - words_compare$prop/words_compare$prop2)
words_compare$dif2 <- 100 * (1 - words_compare$prop2/words_compare$prop)
words_compare$dif <- 0
words_compare$dif[words_compare$dif1 < 0] <- words_compare$dif1[words_compare$dif1 < 0]
words_compare$dif[words_compare$dif2 > 0] <- words_compare$dif2[words_compare$dif2 > 0]
words_compare$dif1 <- NULL
words_compare$dif2 <- NULL
require(ggplot2)
words_compare$z2 <- 0 # insignificant terms
words_compare$z2[abs(words_compare$z) >= 1.96] <- 1 # significant at 95% confidence
words_compare$z2[abs(words_compare$z) >= 2.58] <- 2 # significant at 99% confidence
words_compare$z2[abs(words_compare$z) >= 3.08] <- 3 # significant at 99.79%
words_compare <- words_compare[words_compare$dif >-99 & words_compare$dif <99,]
words_compare <- words_compare[order(abs(words_compare$prop2+words_compare$prop),
decreasing = T),]
ggplot(words_compare, aes(z, dif, colour=z2, label=Row.names)) + geom_point()+
scale_colour_gradientn(name="Z Score", labels=c("Not significant", "1.96", "2.58", "3.08"),colours=rainbow(4))+
geom_text(aes(label=ifelse(words_compare$z == max(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
geom_text(aes(label=ifelse(words_compare$z == min(words_compare$z), as.character(Row.names),'')), hjust=0, vjust=0) +
ylab("Difference")+
xlab("Z Score")+
#theme(panel.background = element_rect(colour =  = "pink"))+
theme(panel.background = element_rect(fill = "black"))+
ggtitle("Relationship between z score and percentage difference \n in Austen and Edgeworth")
ggplot(head(words_compare, 100), aes(dif, log(abs(author_1 + author_2)),size = (author_1 + author_2),label=Row.names, colour = z2, position_jitter(w=0.002, h=0.002)))+
scale_colour_gradientn(colours=rainbow(4))+
geom_text(fontface = 2, alpha = .8) +
scale_size(range = c(2, 10)) +
ylab("log number of mentions") +
xlab("Percentage difference between samples \n <-------More in Edgeworth --------------|--------More in Austen----------->") +
geom_vline(xintercept=0, colour  = "red", linetype=2)+
theme_bw() + theme(legend.position = "none") +
ggtitle("Differences in Terms Used by \n Austen and Edgeworth")
