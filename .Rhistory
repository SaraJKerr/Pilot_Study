+
+	# The following section deals with the case where the user has
+	# entered a word to look up.
+
+	if (Word %in% AllWords) {
+		Hits <- numeric(0)
+		NumHits <- 0
+		Indices <- numeric(0)
+		for (i in 1: TopicCount) {
+			if (Word %in% Phi[[i]]) {
+				NumHits <- NumHits + 1
+				Hits <- c(Hits, which(Phi[[i]] == Word))
+				Indices <- c(Indices, i)
+				}
+			}
+		names(Hits) <- Indices
+		Hits <- sort(Hits, decreasing = FALSE)
+		cat('\n')
+		if (NumHits > 5) NumHits <- 5
+		for (i in 1: NumHits) {
+			Top <- as.integer(names(Hits[i]))
+			cat("Topic", Top, ":", Phi[[Top]][1], Phi[[Top]][2], Phi[[Top]][3], Phi[[Top]][4], Phi[[Top]][5], Phi[[Top]][6], '\n')
+			}
+		User <- readline('Which of these topics do you select? ')
+		if (is.na(TopNum)) TopNum <- 1
+		}
+	else if (Word %in% Documents) {
+		DocIndex <- which(Documents == Word)
+		TopicVector <- Theta[ , DocIndex]
+		names(TopicVector) <- 1:TopicCount
+		TopicVector <- sort(TopicVector, decreasing = TRUE)
+		Top10 <- as.integer(names(TopicVector[1:10]))
+		for (Top in Top10) {
+			cat("Topic", Top, ":", Phi[[Top]][1], Phi[[Top]][2], Phi[[Top]][3], Phi[[Top]][4], Phi[[Top]][5], Phi[[Top]][6], '\n')
+		}
+		User <- readline('Which of these topics do you select? ')
+		if (is.na(TopNum)) TopNum <- 1
+		}
+
+	if (TopNum < 1) TopNum <- 1
+	if (TopNum > TopicCount) TopNum <- TopicCount
+	# By this point we presumably have a valid TopNum.
+
+	cat('\n')
+
+	Freqs <- Theta[TopNum, ]
+	names(Freqs) <- 1:Dimensions[2]
+	Freqs <- sort(Freqs, decreasing = TRUE)
+	Top10 <- as.integer(names(Freqs[1:10]))
+
+	# Generate smoothed curve.
+	Smoothed <- numeric(Timespan)
+	for (i in 1: Timespan) {
+		Start = i-5
+		End = i + 5
+		if (Start < 1) Start = 1
+		if (End > Timespan) End = Timespan
+		Smoothed[i] = mean(ThetaSum[TopNum, Start:End], na.rm = TRUE)
+		}
+
+	Ratio <- max(Theta[TopNum, ])/max(Smoothed)
+	Smoothed <- (Smoothed * Ratio)
+	Range <- 1: Timespan
+	Loess.Smoothed <- loess(Smoothed ~ Range, span = 0.7)
+	Predict.Smoothed <- predict(Loess.Smoothed)
+
+	Selected <- Theta[TopNum, ]
+	Index <- which(Selected > quantile(Selected)[4])
+	Selected <- Selected[Index]
+	SelectLen <- length(Selected)
+	Colours <- character(SelectLen)
+	Shapes <- integer(SelectLen)
+
+	for (i in 1: SelectLen) {
+		Ind <- Index[i]
+		if (Ind %in% Top10) Shapes[i] <- 4
+		else Shapes[i] <- 1
+		Colours[i] <- "gray42"
+		if (Genres[Ind] == "poe") Colours[i] <- "mediumorchid2"
+		if (Genres[Ind] == "bio") Colours[i] <- "gray3"
+		if (Genres[Ind] == "fic") Colours[i] <- "dodgerblue3"
+		if (Genres[Ind] == "dra") Colours[i] <- "olivedrab3"
+		if (Genres[Ind] == "juv") Colours[i] <- "gold1"
+		if (Genres[Ind] == "non") Colours[i] <- "tan4"
+		if (Genres[Ind] == "let" | Genres[Ind] == "ora") {
+			Colours[i] <- "salmon3"
+			Shapes[i] <- 2
+			}
+		}
+	PlotDates <- DocDates[Index]
+
+	plot(PlotDates, Selected, col = Colours, pch = Shapes, xlab = "Blue/fic, purple/poe, green/drama, black/bio, brown/nonfic, triangle/letters or orations.", ylab = "Freq of topic in doc.", main = paste('Topic', TopNum, ':', Phi[[TopNum]][1], Phi[[TopNum]][2], Phi[[TopNum]][3], Phi[[TopNum]][4]))
+	par(new=TRUE)
+	plot(Predict.Smoothed*.7, type = 'l', lwd = 2, col = "gray75", axes = FALSE, ylab = "", xlab = "")
+
+	for (DocNum in Top10) {
+		cat(Documents[DocNum], Authors[DocNum], DocDates[DocNum], '\n')
+		cat(LongTitles[DocNum],'\n','\n')
+		}
+
+	cat('TOPIC', TopNum,':', Phi[[TopNum]][1:50], '\n')
+	cat('OF', TopicCount, 'TOPICS this is #',TopicRanks[TopNum], 'in desc order, with', TopicBulk[TopNum], 'words. Related topics: \n')
+
+	for (i in 1:5) {
+		Top <- KL[[TopNum]][i] + 1
+		cat("Topic", Top, ":", Phi[[Top]][1], Phi[[Top]][2], Phi[[Top]][3], Phi[[Top]][4], Phi[[Top]][5], Phi[[Top]][6], '\n')
+		}
+	cat('\n')
+	}
+
+
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud",
"biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
# Load Texts
cname <- file.path("texts")   # Dir already set to main folder
cname
dir(cname) # Check that texts are in the directory
# Load the package for text mining and then load your texts into R
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Preprocessing text
docs <- tm_map(docs, removePunctuation) # Remove punctuation
docs <- tm_map(docs, removeNumbers) # Remove numbers
docs <- tm_map(docs, tolower) # Convert to lower case
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove stopwords
# Stemming - is this needed, necessary or advisable?
library(SnowballC)
docs <- tm_map(docs, stemDocument)
# Strip white space
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
# Staging the data
dtm <- DocumentTermMatrix(docs)
dtm
inspect(dtm[1:2, 1:20])
dim(dtm) # Shows the number of documents and terms
tdm <- TermDocumentMatrix(docs)   # Transpose of the original matrix
tdm
# Explore the data
# Organize terms by frequency
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
# Export matrix as csv
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="dtm_MP_P.csv")
# Remove sparse terms
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
inspect(dtms)
freq[head(ord)] # Least frequent
freq[tail(ord)] # Most frequent
head(table(freq), 20) # Check frequencies of frequencies low
tail(table(freq), 20) # high
freq <- colSums(as.matrix(dtms))   # Frequency of sparse terms
freq
findFreqTerms(dtm, lowfreq=50) # Identifying terms which appear frequently
wf <- data.frame(word=names(freq), freq=freq)   # Alternative form to line above
head(wf)
# Plot words which appear at least 500 times - can reduce but lose clarity in plot
library(ggplot2)
p <- ggplot(subset(wf, freq>500), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
# Relationships between terms
# Term Correlations
findAssocs(dtm, c("independent" , "miss"), corlimit=0.98)
# specifying a correlation limit of 0.98   - Not sure how useful this is
# Word Cloud
library(wordcloud)
set.seed(142)
wordcloud(names(freq), freq, min.freq=50)
set.seed(142)
wordcloud(names(freq), freq, min.freq=100)
# as above with colour
set.seed(142)
wordcloud(names(freq), freq, min.freq=100, scale=c(5, .1),
colors=brewer.pal(6, "Dark2"))   # 25 is too small!!!
x <- state.x77[1:5,]
x
sort(x$Population)
sort(x[,1])
sort(x)
library(lattice)
barplot(state.x77[1:5, 1])
barplot(state.x77[1:10, 1])
main="Population by state")
barplot(state.x77[1:10, 1], ylab="Population", xlab="State", main="Population by state")
barplot(state.x77[1:10, 1], ylab="Population", xlab="State",
main="Population by state", col=1:10)
plot(mtcars$disp, mtcars$mpg)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG compared with engine displacement")
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1)
head(USArrests)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1, col = 1:5)
plot(mtcars$disp, mtcars$mpg, xlab="Engine displacement", ylab="mpg",
main="MPG vs engine displacement", las=1, col = 1:5, pch=20)
hist(USArrests)
hist(USArrests$Assault)
head(state.x77)
hist(state.x77[1:10,3])
hist(state.x77[,3])
boxplot(iris$Sepal.Length)
boxplot(iris$Sepal.Length~Species)
boxplot(iris$Sepal.Length ~ iris$Species)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=2)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=2)
plot(iris[,-5], pch=20, col=iris[,5], cex=2)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=1)
plot(Sepal.Width ~ Sepal.Length, data = iris, pch=20, col=2:4, cex=6)
head(quakes)
xyplot(lat~long, data=quakes, pch=".")
xyplot(lat~long, data=quakes, pch=".", main="Earthquakes in the Pacific Ocean (since 1964)")
xyplot(lat~long, data=quakes, pch="*", main="Earthquakes in the Pacific Ocean (since 1964)")
head(barley)
barchart(yield ~ variety | site, data = barley,
groups = year, layout = c(1,6),
ylab = "Barley Yield (bushels/acre)",
scales = list(x = list(abbreviate = TRUE,
minlength = 5)))
scales = list(x = list(abbreviate = F,
barchart(yield ~ variety | site, data = barley,
groups = year, layout = c(1,6),
ylab = "Barley Yield (bushels/acre)",
scales = list(x = list(abbreviate = F,
minlength = 5)))
dotplot(variety ~ yield | year * site, data=barley)
dotplot(variety ~ yield | site, data = barley, groups = year,
key = simpleKey(levels(barley$year), space = "right"),
xlab = "Barley Yield (bushels/acre) ",
aspect=0.5, layout = c(1,6), ylab=NULL)
head(singer)
hist(singer$height)
hist(singer$height ~ singer$voice.part)
histogram( ~ height | voice.part, data = singer, nint = 17,
endpoints = c(59.5, 76.5), layout = c(2,4), aspect = 1,
xlab = "Height (inches)")
histogram( ~ height | voice.part, data = singer,
endpoints = c(59.5, 76.5), layout = c(2,4), aspect = 1,
xlab = "Height (inches)")
stripplot(voice.part ~ jitter(height), data = singer, aspect = 1,
jitter = TRUE, xlab = "Height (inches)")
stripplot(voice.part ~ jitter(height), data = singer, aspect = 1,
jitter = TRUE, xlab = "Height (inches)")
head(chickwts)
hist(chickwts$weight)
hist(chickwts$weight, col="red", xlab="Weight", main = "Histogram of Chick Weights")
data()
xyplot(Sepal.Length + Sepal.Width ~ Petal.Length + Petal.Width | Species,
data = iris, scales = "free", layout = c(2, 2),
auto.key = list(x = .6, y = .7, corner = c(0, 0)))
library(lattice)
xyplot(Sepal.Length + Sepal.Width ~ Petal.Length + Petal.Width | Species,
data = iris, scales = "free", layout = c(2, 2),
auto.key = list(x = .6, y = .7, corner = c(0, 0)))
wireframe(volcano, shade = TRUE,
aspect = c(61/87, 0.4),
light.source = c(10,0,10))
head(volcano)
x <- c(66.5,22,58.5,70.5,67.5,51,68.5,60,63.5,87,62,58.5,61.5,68.5,63.5,56.5,55,51,54.5)
x <- c(66.5,22,58.5,70.5,67.5,51,68.5,60,63.5,87,62,58.5,61.5,68.5,63.5,56.5,55,51,54.5)
sort(x)
y <-sort(x)
plot(y)
hist(y)
hist(y[-1])
hist(y[-1], breaks=10)
hist(y[-1], breaks=20)
mtcars[which(mtcars$disp == 472), ]
install.packages("scatterplot3D")
install.packages("scatterplot3d")
library(scatterplot3d)
x <- c(1:5)
y <- c(1:5)
z <- c(1:5)
scatterplot3d(x, y, z, color = "blue", pch = 19, main = "Data plotted in 3D")
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 6), ylim = c(0, 6),
zlim = c(0, 6), main = "Data plotted in 3D")
x <- c(1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4)
y <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)
z <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 6), ylim = c(0, 6),
zlim = c(0, 6), main = "Data plotted in 3D")
x <- c(1,2,3)
y <- c(1,2,3)
z <- c(1,2,3)
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 4), ylim = c(0, 4),
zlim = c(0, 4), main = "Data plotted in 3D")
x <- c(1,2,3,1,1)
y <- c(1,2,3,2,1)
z <- c(1,2,3,1,2)
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 4), ylim = c(0, 4),
zlim = c(0, 4), main = "Data plotted in 3D")
x <- c(1,2,3,1,1,1,1,1)
y <- c(1,2,3,2,1,2,2,3)
z <- c(1,2,3,1,2,2,3,3)
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 4), ylim = c(0, 4),
zlim = c(0, 4), main = "Data plotted in 3D")
x <- c(1,2,1,1,2,2)
y <- c(1,2,1,2,1,1)
z <- c(1,2,2,2,2,1)
scatterplot3d(x, y, z, color = "blue", pch = 19, xlim = c(0, 3), ylim = c(0, 3),
zlim = c(0, 3), main = "Data plotted in 3D")
library(RNeo4j)
graph = startGraph("http://localhost:7474/db/data/", username = "neo4j",
password = "phd-1922-cD42")
clear(graph)
library(RNeo4j)
graph = startGraph("http://localhost:7474/db/data/", username = "neo4j",
password = "phd-1922-cD42")
clear(graph)
plot(g, layout=layout_with_kk)
setwd("~/PhD_Main/Pilot/Pilot_Study")
library(tm)
setwd("~/PhD_Main/Pilot/Pilot_Study/Data/Corpus")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
docs <- Corpus(VectorSource(files))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
write.csv(freq[ord], "Results/Word_freq_.csv")
setwd("~/PhD_Main/Pilot/Pilot_Study")
write.csv(freq[ord], "Results/Word_freq_.csv")
library(ldatuning)
library(topicmodels)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 42),
mc.cores = 2L, # Check this with About this Mac
verbose = TRUE
)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 50, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 42),
mc.cores = 2L, # Check this with About this Mac
verbose = TRUE
)
library(ldatuning)
library(topicmodels)
library(doParallel)
library(ggplot2)
library(scales)
library(tidyverse)
library(RColorBrewer)
library(wordcloud)
install.packages("tidyverse")
library(tidyverse)
cosineSim <- function(x){
as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cs <- cosineSim(m)
csm <- as.matrix(cs)
c <- as.vector(csm)
cs <- cosineSim(m)
dname <- file.path("Data/Corpus") # Tells R the path to the files
dname
dir(dname)
ja_me_ot <- dir(dname)
m <- as.matrix(dtm)
rownames(m) <- ja_me_ot
#compute Euclidean distance between document vectors
d <- dist(m)
cs <- cosineSim(m)
csm <- as.matrix(cs)
c <- as.vector(csm)
c <- c[which(c != 0)]
library(igraph)
g <- graph_from_incidence_matrix(csm)
edges <- get.edgelist(g)
colnames(edges) <- c("from", "to")
edf <- as.data.frame(edges)
edf$weight <- c
edf2 <- subset(edf, weight > 0.5)
edf2 <- subset(edf, weight > 0.6)
edf2 <- as.matrix(edf2)
g2 <- graph(edges = edf2[, -3])
g2 <- simplify(g2)
E(g2)$arrow.size <- 0.05
g2.sym <- as.undirected(g2, mode="collapse",
edge.attr.comb=list(weight="sum", "ignore"))
ceb <- cluster_edge_betweenness(g2)
dendPlot(ceb, mode="hclust")
plot(ceb, g2)
head(dtm)
View(edf)
V(g2)$id <- 1:30
V(g2)$id
V(g2)$name
View(edges)
View(edf)
edf2 <- subset(edf, weight > 0.5)
edf2 <- as.matrix(edf2)
g2 <- graph(edges = edf2[, -3])
g2 <- simplify(g2)
E(g2)$arrow.size <- 0.05
V(g2)$id <- 1:30
ceb <- cluster_edge_betweenness(g2)
dendPlot(ceb, mode="hclust")
l <- layout_with_fr(g2)
plot(ceb, g2, layout = l)
l <- layout_with_kk(g2)
plot(ceb, g2, layout = l)
plot(ceb, g2, layout = l, vertex.labels = id)
plot(ceb, g2, layout = l, vertex.label = id)
plot(ceb, g2, layout = l, vertex.label.font = 2)
plot(ceb, g2, layout = l, vertex.label.font = 0.5)
plot(ceb, g2, layout = l, vertex.label.font = 0.1)
plot(ceb, g2, layout = l, vertex.label.font = 0.05)
plot(ceb, g2, layout = l, vertex.label.font = 1)
plot(ceb, g2, layout = l, vertex.label.font = 2)
plot(ceb, g2)
plot(ceb, g2, vertex.label.font = 0.2)
plot(ceb, g2, vertex.label.font = 0.1)
plot(ceb, g2, vertex.label = V(g2)$id)
plot(ceb, g2, vertex.label = V(g2)$id, main = "Cluster Edge Betweenness")
plot(ceb, g2, vertex.label = V(g2)$id, main = "Cluster Edge Betweenness",
legend = V(g2)$name)
cfg <- cluster_fast_greedy(as.undirected(g2))
plot(cfg, as.undirected(g2))
plot(cfg, as.undirected(g2), vertex.label = V(g2)$id,
main = "Cluster Fast Greedy" )
V(g2)$community <- ceb$membership
pal2 <- rainbow(10, alpha = 0.5)
plot(g2, vertex.color=pal2[V(g2)$community])
plot(g2, vertex.color=pal2[V(g2)$community], vertex.label = V(g2)$id,
main = "Cluster Edge Betweenness 2")
pal2 <- rainbow(5, alpha = 0.7)
plot(g2, vertex.color=pal2[V(g2)$community], vertex.label = V(g2)$id,
main = "Cluster Edge Betweenness 2")
V(g2)$color <- pal2[V(g2)$community]
library(visNetwork)
visIgraph(g2)
visIgraph(g2) %>% visSave("JA_ME_OT_30.html")
length(freq)
burnin = 1000
iter = 1000
keep = 50
full_data  <- dtm[6:11 , ]
n <- nrow(full_data)
k <- 5 # number of topics
splitter <- sample(1:n, round(n * 0.75))
train_set <- full_data[splitter, ]
valid_set <- full_data[-splitter, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
control = list(burnin = burnin, iter = iter, keep = keep) )
perplexity(fitted, newdata = train_set)
perplexity(fitted, newdata = valid_set)
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
clusterExport(cluster, c("full_data", "k", "burnin", "iter",
"keep", "splitfolds"))
results <- foreach(i = 1:folds) %dopar% {
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
control = list(burnin = burnin, iter = iter, keep = keep))
return(perplexity(fitted, newdata = valid_set))
}
stopCluster(cluster)
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 15, 20, 25, 30, 50, 75, 100,) # how many topics
candidate_k <- c(2, 3, 4, 5, 10, 15, 20, 25, 30, 50, 75, 100) # how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep",
"splitfolds", "folds", "candidate_k"))
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
control = list(burnin = burnin, iter = iter, keep = keep) )
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
